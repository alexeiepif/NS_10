{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u81YRkk6Y8su"
   },
   "source": [
    "1. На основе учебного ноутбука проведите финальную подготовку данных. Иизмените количество сегментирующих классов с `16` на `5`.\n",
    "\n",
    "2. Проведите суммарно не менее `10` экспериментов и визуализируйте их результаты (включая точность обучения сетей на одинаковом количестве эпох, например, на `7`):\n",
    "\n",
    "  - изменив `filters` в сверточных слоях\n",
    "  - изменив `kernel_size` в сверточных слоях\n",
    "  - изменив активационную функцию в скрытых слоях с `relu` на `linear` или/и `selu`, `elu`.\n",
    "\n",
    "\n",
    "**Важно!**\n",
    "\n",
    "Многие эксперименты могут приводить к переполнению ОЗУ в вашем ноутбуке и сброса кода обучения.\n",
    "\n",
    "![image.png](data:image/png;base64,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)\n",
    "\n",
    "\n",
    "\n",
    "Для предотвращения переполнения ОЗУ может помочь библиотека `gc`. Вставьте строчку `gc.collect()` в цикл ваших экспериментов для сбора и удаления временных данных (кеш)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OzYOKrjeRpzw"
   },
   "source": [
    "Перед выполнением задания, пожалуйста, запустите ячейку `Подготовка` ниже:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oFyI8tjLV1ba"
   },
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P3LPRx8H9v_c"
   },
   "source": [
    "### Импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KymN8bdebLzJ"
   },
   "outputs": [],
   "source": [
    "# очистка ОЗУ\n",
    "import gc\n",
    "\n",
    "# Для работы с множествами\n",
    "import itertools\n",
    "\n",
    "# Для работы с файлами\n",
    "import os\n",
    "\n",
    "# Для работы со временем\n",
    "import time\n",
    "\n",
    "# Для работы с арихивами\n",
    "import zipfile\n",
    "\n",
    "# загрузка файлов по HTML ссылке\n",
    "import gdown\n",
    "\n",
    "# Импортируем модуль pyplot библиотеки matplotlib для построения графиков\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Импортируем библиотеку numpy\n",
    "import numpy as np\n",
    "\n",
    "# Для работы с таблицами\n",
    "import pandas as pd\n",
    "\n",
    "# Для вывода в Jupyter Notebook\n",
    "from IPython.display import display\n",
    "\n",
    "# Импортируем стандартные слои keras\n",
    "from tensorflow.keras.layers import (\n",
    "    Activation,\n",
    "    BatchNormalization,\n",
    "    Conv2D,\n",
    "    Conv2DTranspose,\n",
    "    Input,\n",
    "    MaxPooling2D,\n",
    "    concatenate,\n",
    ")\n",
    "\n",
    "# Импортируем модели keras: Model\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Импортируем оптимизатор Adam\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Импортируем модуль image для работы с изображениями\n",
    "from tensorflow.keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bVjxntar-a5H"
   },
   "source": [
    "### Загрузка датасета\n",
    "\n",
    "грузим и распаковываем архив картинок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xP4-NkAt96gv",
    "outputId": "16a43153-fbf2-47f0-c540-b0c74e6d5655"
   },
   "outputs": [],
   "source": [
    "# Загрузка датасета из облака\n",
    "if not os.path.exists(\"construction_256x192.zip\"):\n",
    "    gdown.download(\n",
    "        \"https://storage.yandexcloud.net/aiueducation/Content/base/l14/construction_256x192.zip\",\n",
    "        None,\n",
    "        quiet=False,\n",
    "    )\n",
    "\n",
    "# распоковываем архив\n",
    "if not os.path.exists(\"train\") or not os.path.exists(\"val\"):\n",
    "    with zipfile.ZipFile(\"construction_256x192.zip\", \"r\") as zip_ref:\n",
    "        zip_ref.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Настрйока глобальных параметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CbNtynGfV27x"
   },
   "outputs": [],
   "source": [
    "# Глобальные параметры\n",
    "IMG_WIDTH = 256               # Ширина картинки\n",
    "IMG_HEIGHT = 192              # Высота картинки\n",
    "NUM_CLASSES = 5               # Задаем количество классов на изображении\n",
    "TRAIN_DIRECTORY = 'train'     # Название папки с файлами обучающей выборки\n",
    "VAL_DIRECTORY = 'val'         # Название папки с файлами проверочной выборки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготовка выборок"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vt2yMYbdrTVP"
   },
   "source": [
    "Загрузим оригинальные изображения (код из лекции):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MRDvv0-DQHwT",
    "outputId": "39502039-5591-45e6-f87f-0bc3c8a875c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучающая выборка загружена. Время загрузки: 0.18c\n",
      "Количество изображений:  1900\n",
      "Проверочная выборка загружена. Время загрузки: 0.01c\n",
      "Количество изображений:  100\n"
     ]
    }
   ],
   "source": [
    "train_images = []  # Создаем пустой список для хранений оригинальных изображений обучающей выборки\n",
    "val_images = []  # Создаем пустой список для хранений оригинальных изображений проверочной выборки\n",
    "\n",
    "cur_time = time.time()  # Засекаем текущее время\n",
    "\n",
    "# Проходим по всем файлам в каталоге по указанному пути\n",
    "for filename in sorted(os.listdir(TRAIN_DIRECTORY + \"/original\")):\n",
    "    # Читаем очередную картинку и добавляем ее в список изображений с указанным target_size\n",
    "    train_images.append(\n",
    "        image.load_img(\n",
    "            os.path.join(TRAIN_DIRECTORY + \"/original\", filename),\n",
    "            target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Отображаем время загрузки картинок обучающей выборки\n",
    "print(\n",
    "    \"Обучающая выборка загружена. Время загрузки: \",\n",
    "    round(time.time() - cur_time, 2),\n",
    "    \"c\",\n",
    "    sep=\"\",\n",
    ")\n",
    "\n",
    "# Отображаем количество элементов в обучающей выборке\n",
    "print(\"Количество изображений: \", len(train_images))\n",
    "\n",
    "cur_time = time.time()  # Засекаем текущее время\n",
    "\n",
    "# Проходим по всем файлам в каталоге по указанному пути\n",
    "for filename in sorted(os.listdir(VAL_DIRECTORY + \"/original\")):\n",
    "    # Читаем очередную картинку и добавляем ее в список изображений с указанным target_size\n",
    "    val_images.append(\n",
    "        image.load_img(\n",
    "            os.path.join(VAL_DIRECTORY + \"/original\", filename),\n",
    "            target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Отображаем время загрузки картинок проверочной выборки\n",
    "print(\n",
    "    \"Проверочная выборка загружена. Время загрузки: \",\n",
    "    round(time.time() - cur_time, 2),\n",
    "    \"c\",\n",
    "    sep=\"\",\n",
    ")\n",
    "\n",
    "# Отображаем количество элементов в проверочной выборке\n",
    "print(\"Количество изображений: \", len(val_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ICqodxfPra3k"
   },
   "source": [
    "Загрузим сегментированные изображения (код из лекции):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tyA-q3d5YOL5",
    "outputId": "07f7f7b2-dfe7-49b6-d48f-55087b15b04a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучающая выборка загружена. Время загрузки: 0.18c\n",
      "Количество изображений:  1900\n",
      "Проверочная выборка загружена. Время загрузки: 0.01c\n",
      "Количество изображений:  100\n",
      "Проверочная выборка загружена. Время загрузки: 0.01c\n",
      "Количество изображений:  100\n"
     ]
    }
   ],
   "source": [
    "train_segments = []  # Создаем пустой список для хранений оригинальных изображений обучающей выборки\n",
    "val_segments = []  # Создаем пустой список для хранений оригинальных изображений проверочной выборки\n",
    "\n",
    "cur_time = time.time()  # Засекаем текущее время\n",
    "\n",
    "for filename in sorted(\n",
    "    os.listdir(TRAIN_DIRECTORY + \"/segment\")\n",
    "):  # Проходим по всем файлам в каталоге по указанному пути\n",
    "    # Читаем очередную картинку и добавляем ее в список изображений с указанным target_size\n",
    "    train_segments.append(\n",
    "        image.load_img(\n",
    "            os.path.join(TRAIN_DIRECTORY + \"/segment\", filename),\n",
    "            target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Отображаем время загрузки картинок обучающей выборки\n",
    "print(\n",
    "    \"Обучающая выборка загружена. Время загрузки: \",\n",
    "    round(time.time() - cur_time, 2),\n",
    "    \"c\",\n",
    "    sep=\"\",\n",
    ")\n",
    "\n",
    "# Отображаем количество элементов в обучающем наборе сегментированных изображений\n",
    "print(\"Количество изображений: \", len(train_segments))\n",
    "\n",
    "cur_time = time.time()  # Засекаем текущее время\n",
    "\n",
    "for filename in sorted(\n",
    "    os.listdir(VAL_DIRECTORY + \"/segment\")\n",
    "):  # Проходим по всем файлам в каталоге по указанному пути\n",
    "    # Читаем очередную картинку и добавляем ее в список изображений с указанным target_size\n",
    "    val_segments.append(\n",
    "        image.load_img(\n",
    "            os.path.join(VAL_DIRECTORY + \"/segment\", filename),\n",
    "            target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Отображаем время загрузки картинок проверочной выборки\n",
    "print(\n",
    "    \"Проверочная выборка загружена. Время загрузки: \",\n",
    "    round(time.time() - cur_time, 2),\n",
    "    \"c\",\n",
    "    sep=\"\",\n",
    ")\n",
    "\n",
    "# Отображаем количество элементов в проверочном наборе сегментированных изображений\n",
    "print(\"Количество изображений: \", len(val_segments))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NQdPRE5OYLE-"
   },
   "source": [
    "## Решение\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание констант"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-3yC0JWQRF-g"
   },
   "outputs": [],
   "source": [
    "# Ваше решение\n",
    "CLASS_LABELS_16 = (\n",
    "    (100, 100, 100),  # Пол (серый)\n",
    "    (0, 0, 100),  # Потолок (синий)\n",
    "    (0, 100, 0),  # Стена (зеленый)\n",
    "    (100, 0, 0),  # Колонна (красный)\n",
    "    (0, 100, 100),  # Проем (темно-бирюзовый)\n",
    "    (100, 0, 100),  # Дверь (бордовый)\n",
    "    (100, 100, 0),  # Окно (золотой)\n",
    "    (200, 200, 200),  # Внешний мир (светло-серый)\n",
    "    (0, 200, 0),  # Перила (светло-зеленый)\n",
    "    (200, 0, 0),  # Батареи (светло-красный)\n",
    "    (0, 200, 200),  # Люди (бирюзовый)\n",
    "    (0, 0, 200),  # Лестница (светло-синий)\n",
    "    (200, 0, 200),  # Инвентарь (розовый)\n",
    "    (200, 200, 0),  # Лампа (желтый)\n",
    "    (0, 100, 200),  # Провод (голубой)\n",
    "    (100, 0, 200),  # Балка (фиолетовый)\n",
    ")\n",
    "LABELS_5_CLASS = (0, 1, 2, *[3] * 9, 4, *[3] * 3) # Сокращаем до 5 классов\n",
    "FLOOR = CLASS_LABELS_16[0] # Пол\n",
    "CEILING = CLASS_LABELS_16[1] # Потолок\n",
    "WALL = CLASS_LABELS_16[2] # Стена\n",
    "INVENTORY = (200, 0, 200) # Инвентарь\n",
    "OTHER = (100, 0, 0) # Другие объекты\n",
    "CLASS_LABELS_5 = (FLOOR, CEILING, WALL, INVENTORY, OTHER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функции преобразования RGB в метки классов и обратно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb_to_labels_5(image_list, class_labels=CLASS_LABELS_16, new_labels=CLASS_LABELS_5):\n",
    "    # Преобразует RGB-картинки сегментации в метки классов (5 классов)\n",
    "    result = []\n",
    "    for d in image_list:\n",
    "        sample = np.array(d)\n",
    "        y = np.zeros((IMG_HEIGHT, IMG_WIDTH, 1), dtype=\"uint8\")\n",
    "\n",
    "        for i, cl in enumerate(class_labels):\n",
    "            # Находим пиксели с цветом cl и присваиваем им новый класс\n",
    "            y[np.where(np.all(sample == class_labels[i], axis=-1))] = new_labels[i]\n",
    "        result.append(y)\n",
    "    return np.array(result)\n",
    "\n",
    "\n",
    "def labels_5_to_rgb(image_list):\n",
    "    # Преобразует метки классов обратно в RGB-картинки (5 цветов)\n",
    "    result = []\n",
    "    for y in image_list:\n",
    "        temp = np.zeros((IMG_HEIGHT, IMG_WIDTH, 3), dtype=\"uint8\")\n",
    "        for i, cl in enumerate(CLASS_LABELS_5):\n",
    "            # Находим пиксели с меткой i и присваиваем им цвет из CLASS_LABELS_5\n",
    "            temp[np.where(np.all(y == i, axis=-1))] = CLASS_LABELS_5[i]\n",
    "        result.append(temp)\n",
    "    return np.array(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготовка выборок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Форма x_train: (1900, 192, 256, 3)\n",
      "Форма x_val: (100, 192, 256, 3)\n",
      "Форма y_train: (1900, 192, 256, 1)\n",
      "Форма y_val: (100, 192, 256, 1)\n"
     ]
    }
   ],
   "source": [
    "# Создание выборок для обучения и проверки\n",
    "x_train = np.array([image.img_to_array(img) for img in train_images])\n",
    "x_val = np.array([image.img_to_array(img) for img in val_images])\n",
    "\n",
    "# Преобразование сегментированных изображений в метки классов (5 классов)\n",
    "y_train = rgb_to_labels_5(train_segments, CLASS_LABELS_16, LABELS_5_CLASS)\n",
    "y_val = rgb_to_labels_5(val_segments, CLASS_LABELS_16, LABELS_5_CLASS)\n",
    "\n",
    "print(\"Форма x_train:\", x_train.shape)\n",
    "print(\"Форма x_val:\", x_val.shape)\n",
    "print(\"Форма y_train:\", y_train.shape)\n",
    "print(\"Форма y_val:\", y_val.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функция вывода примеров работы модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_images(model, count=1):\n",
    "    # Функцтя для вывода результатов работы модели \n",
    "    # на случайных изображениях из валидационной выборки\n",
    "    indexes = np.random.randint(0, len(x_val), count)\n",
    "    predict = np.argmax(model.predict(x_val[indexes]), axis=-1)\n",
    "    orig = labels_5_to_rgb(predict[..., None])\n",
    "\n",
    "    fig, axs = plt.subplots(3, count, figsize=(25, 15))\n",
    "\n",
    "    if count == 1:\n",
    "        axs[0].set_title(\"Результат работы модели:\")\n",
    "        axs[0].imshow(orig[0])\n",
    "        axs[0].axis(\"off\")\n",
    "\n",
    "        axs[1].set_title(\"Оригинальное сегментированное\")\n",
    "        axs[1].imshow(val_segments[indexes[0]])\n",
    "        axs[1].axis(\"off\")\n",
    "\n",
    "        axs[2].set_title(\"Оригинальное изображение\")\n",
    "        axs[2].imshow(val_images[indexes[0]])\n",
    "        axs[2].axis(\"off\")\n",
    "    else:\n",
    "        for i in range(count):\n",
    "            axs[0, 0].set_title(\"Результат работы модели:\")\n",
    "            axs[0, i].imshow(orig[i])\n",
    "            axs[0, i].axis(\"off\")\n",
    "\n",
    "            axs[1, 0].set_title(\"Оригинальное сегментированное\")\n",
    "            axs[1, i].imshow(val_segments[indexes[i]])\n",
    "            axs[1, i].axis(\"off\")\n",
    "\n",
    "            axs[2, 0].set_title(\"Оригинальное изображение\")\n",
    "            axs[2, i].imshow(val_images[indexes[i]])\n",
    "            axs[2, i].axis(\"off\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создание и обучение моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_unet(\n",
    "    CLASS_COUNT, input_shape, filters=64, kernel_size=(3, 3), activation=\"relu\"\n",
    "):\n",
    "    # Входной слой\n",
    "    img_input = Input(input_shape)\n",
    "\n",
    "    # Первый блок свертки\n",
    "    x = Conv2D(filters, kernel_size, padding=\"same\", activation=activation)(img_input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(filters, kernel_size, padding=\"same\", activation=activation)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x_1 = x\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "\n",
    "    # Второй блок свертки\n",
    "    x = Conv2D(filters * 2, kernel_size, padding=\"same\", activation=activation)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(filters * 2, kernel_size, padding=\"same\", activation=activation)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x_2 = x\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "\n",
    "    # Третий блок свертки\n",
    "    x = Conv2D(filters * 4, kernel_size, padding=\"same\", activation=activation)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(filters * 4, kernel_size, padding=\"same\", activation=activation)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x_3 = x\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "\n",
    "    # Четвертый блок свертки\n",
    "    x = Conv2D(filters * 8, kernel_size, padding=\"same\", activation=activation)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(filters * 8, kernel_size, padding=\"same\", activation=activation)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x_4 = x\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "\n",
    "    # Пятый блок свертки\n",
    "    x = Conv2D(filters * 16, kernel_size, padding=\"same\", activation=activation)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(filters * 16, kernel_size, padding=\"same\", activation=activation)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # Блок декодирования\n",
    "    x = Conv2DTranspose(\n",
    "        filters * 8, kernel_size, strides=(2, 2), padding=\"same\", activation=activation\n",
    "    )(x)\n",
    "\n",
    "    # Первый блок декодирования\n",
    "    x = BatchNormalization()(x)\n",
    "    x = concatenate([x, x_4])\n",
    "    x = Conv2D(filters * 8, kernel_size, padding=\"same\", activation=activation)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(filters * 8, kernel_size, padding=\"same\", activation=activation)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # Второй блок декодирования\n",
    "    x = Conv2DTranspose(\n",
    "        filters * 4, kernel_size, strides=(2, 2), padding=\"same\", activation=activation\n",
    "    )(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = concatenate([x, x_3])\n",
    "    x = Conv2D(filters * 4, kernel_size, padding=\"same\", activation=activation)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(filters * 4, kernel_size, padding=\"same\", activation=activation)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # Третий блок декодирования\n",
    "    x = Conv2DTranspose(\n",
    "        filters * 2, kernel_size, strides=(2, 2), padding=\"same\", activation=activation\n",
    "    )(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = concatenate([x, x_2])\n",
    "    x = Conv2D(filters * 2, kernel_size, padding=\"same\", activation=activation)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(filters * 2, kernel_size, padding=\"same\", activation=activation)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # Четвертый блок декодирования\n",
    "    x = Conv2DTranspose(\n",
    "        filters, kernel_size, strides=(2, 2), padding=\"same\", activation=activation\n",
    "    )(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = concatenate([x, x_1])\n",
    "    x = Conv2D(filters, kernel_size, padding=\"same\", activation=activation)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(filters, kernel_size, padding=\"same\", activation=activation)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # Выходной слой\n",
    "    x = Conv2D(CLASS_COUNT, (1, 1), padding=\"same\")(x)\n",
    "    x = Activation(\"softmax\")(x)\n",
    "\n",
    "    model = Model(img_input, x)  # Создаем модель\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=1e-3),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"sparse_categorical_accuracy\"],\n",
    "    )  # Компилируем модель\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создание списков параметров моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters_list = [32, 64]\n",
    "kernel_size_list = [(3, 3), (7, 7)]\n",
    "activation_list = [\"relu\", \"linear\", \"selu\"]\n",
    "\n",
    "experiment_results = []\n",
    "\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Цикл создания и обучения моделей с различными параметрами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Эксперимент с filters=32, kernel_size=(3, 3), activation='relu'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 157ms/step - loss: 1.4053 - sparse_categorical_accuracy: 0.4700 - val_loss: 78.1390 - val_sparse_categorical_accuracy: 0.3603\n",
      "Epoch 2/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - loss: 0.8524 - sparse_categorical_accuracy: 0.6977 - val_loss: 2.6209 - val_sparse_categorical_accuracy: 0.5242\n",
      "Epoch 3/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - loss: 0.7401 - sparse_categorical_accuracy: 0.7350 - val_loss: 1.4806 - val_sparse_categorical_accuracy: 0.6497\n",
      "Epoch 4/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - loss: 0.6933 - sparse_categorical_accuracy: 0.7493 - val_loss: 1.4731 - val_sparse_categorical_accuracy: 0.5996\n",
      "Epoch 5/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - loss: 0.6598 - sparse_categorical_accuracy: 0.7599 - val_loss: 1.2071 - val_sparse_categorical_accuracy: 0.6013\n",
      "Epoch 6/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - loss: 0.6144 - sparse_categorical_accuracy: 0.7762 - val_loss: 0.9552 - val_sparse_categorical_accuracy: 0.6768\n",
      "Epoch 7/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - loss: 0.5899 - sparse_categorical_accuracy: 0.7864 - val_loss: 0.9189 - val_sparse_categorical_accuracy: 0.6631\n",
      "Epoch 8/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - loss: 0.5776 - sparse_categorical_accuracy: 0.7893 - val_loss: 0.8985 - val_sparse_categorical_accuracy: 0.6798\n",
      "Epoch 9/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - loss: 0.5339 - sparse_categorical_accuracy: 0.8061 - val_loss: 1.0213 - val_sparse_categorical_accuracy: 0.6808\n",
      "Epoch 10/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - loss: 0.5246 - sparse_categorical_accuracy: 0.8089 - val_loss: 0.9405 - val_sparse_categorical_accuracy: 0.6891\n",
      "\n",
      "Эксперимент с filters=32, kernel_size=(3, 3), activation='linear'\n",
      "Epoch 1/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 89ms/step - loss: 1.4033 - sparse_categorical_accuracy: 0.4682 - val_loss: 30.2820 - val_sparse_categorical_accuracy: 0.4495\n",
      "Epoch 2/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - loss: 0.9413 - sparse_categorical_accuracy: 0.6605 - val_loss: 6.9024 - val_sparse_categorical_accuracy: 0.4180\n",
      "Epoch 3/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - loss: 0.8416 - sparse_categorical_accuracy: 0.6918 - val_loss: 3.7587 - val_sparse_categorical_accuracy: 0.4733\n",
      "Epoch 4/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - loss: 0.8222 - sparse_categorical_accuracy: 0.6925 - val_loss: 2.6357 - val_sparse_categorical_accuracy: 0.4586\n",
      "Epoch 5/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - loss: 0.7926 - sparse_categorical_accuracy: 0.7047 - val_loss: 1.2669 - val_sparse_categorical_accuracy: 0.6061\n",
      "Epoch 6/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - loss: 0.7592 - sparse_categorical_accuracy: 0.7178 - val_loss: 1.7280 - val_sparse_categorical_accuracy: 0.5363\n",
      "Epoch 7/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - loss: 0.7540 - sparse_categorical_accuracy: 0.7194 - val_loss: 1.2170 - val_sparse_categorical_accuracy: 0.6242\n",
      "Epoch 8/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - loss: 0.7621 - sparse_categorical_accuracy: 0.7153 - val_loss: 1.0590 - val_sparse_categorical_accuracy: 0.6331\n",
      "Epoch 9/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - loss: 0.7369 - sparse_categorical_accuracy: 0.7244 - val_loss: 1.1194 - val_sparse_categorical_accuracy: 0.6272\n",
      "Epoch 10/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - loss: 0.7273 - sparse_categorical_accuracy: 0.7303 - val_loss: 1.0094 - val_sparse_categorical_accuracy: 0.6355\n",
      "\n",
      "Эксперимент с filters=32, kernel_size=(3, 3), activation='selu'\n",
      "Epoch 1/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 87ms/step - loss: 1.3661 - sparse_categorical_accuracy: 0.4888 - val_loss: 24.0223 - val_sparse_categorical_accuracy: 0.3071\n",
      "Epoch 2/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - loss: 0.8635 - sparse_categorical_accuracy: 0.6939 - val_loss: 3.0865 - val_sparse_categorical_accuracy: 0.4062\n",
      "Epoch 3/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - loss: 0.7521 - sparse_categorical_accuracy: 0.7264 - val_loss: 1.3839 - val_sparse_categorical_accuracy: 0.5426\n",
      "Epoch 4/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - loss: 0.7090 - sparse_categorical_accuracy: 0.7388 - val_loss: 1.5772 - val_sparse_categorical_accuracy: 0.5361\n",
      "Epoch 5/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - loss: 0.6724 - sparse_categorical_accuracy: 0.7533 - val_loss: 0.9523 - val_sparse_categorical_accuracy: 0.6526\n",
      "Epoch 6/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - loss: 0.6349 - sparse_categorical_accuracy: 0.7642 - val_loss: 1.0017 - val_sparse_categorical_accuracy: 0.6517\n",
      "Epoch 7/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - loss: 0.6023 - sparse_categorical_accuracy: 0.7806 - val_loss: 1.0840 - val_sparse_categorical_accuracy: 0.6091\n",
      "Epoch 8/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - loss: 0.5854 - sparse_categorical_accuracy: 0.7844 - val_loss: 0.9571 - val_sparse_categorical_accuracy: 0.6489\n",
      "Epoch 9/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - loss: 0.5527 - sparse_categorical_accuracy: 0.7969 - val_loss: 1.0514 - val_sparse_categorical_accuracy: 0.6157\n",
      "Epoch 10/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - loss: 0.5407 - sparse_categorical_accuracy: 0.8000 - val_loss: 1.1596 - val_sparse_categorical_accuracy: 0.5717\n",
      "\n",
      "Эксперимент с filters=32, kernel_size=(7, 7), activation='relu'\n",
      "Epoch 1/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 242ms/step - loss: 1.4397 - sparse_categorical_accuracy: 0.4717 - val_loss: 1356.5331 - val_sparse_categorical_accuracy: 0.1692\n",
      "Epoch 2/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - loss: 0.9516 - sparse_categorical_accuracy: 0.6578 - val_loss: 2.7819 - val_sparse_categorical_accuracy: 0.2694\n",
      "Epoch 3/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - loss: 0.8443 - sparse_categorical_accuracy: 0.6976 - val_loss: 2.0084 - val_sparse_categorical_accuracy: 0.5400\n",
      "Epoch 4/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - loss: 0.7748 - sparse_categorical_accuracy: 0.7180 - val_loss: 1.2331 - val_sparse_categorical_accuracy: 0.5128\n",
      "Epoch 5/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - loss: 0.7234 - sparse_categorical_accuracy: 0.7356 - val_loss: 1.1791 - val_sparse_categorical_accuracy: 0.5748\n",
      "Epoch 6/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - loss: 0.7212 - sparse_categorical_accuracy: 0.7344 - val_loss: 0.9726 - val_sparse_categorical_accuracy: 0.6199\n",
      "Epoch 7/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - loss: 0.6857 - sparse_categorical_accuracy: 0.7478 - val_loss: 1.1668 - val_sparse_categorical_accuracy: 0.5947\n",
      "Epoch 8/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - loss: 0.6654 - sparse_categorical_accuracy: 0.7534 - val_loss: 0.9013 - val_sparse_categorical_accuracy: 0.6835\n",
      "Epoch 9/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - loss: 0.6394 - sparse_categorical_accuracy: 0.7664 - val_loss: 1.0868 - val_sparse_categorical_accuracy: 0.5695\n",
      "Epoch 10/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - loss: 0.6209 - sparse_categorical_accuracy: 0.7724 - val_loss: 1.3210 - val_sparse_categorical_accuracy: 0.5444\n",
      "\n",
      "Эксперимент с filters=32, kernel_size=(7, 7), activation='linear'\n",
      "Epoch 1/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 126ms/step - loss: 1.5059 - sparse_categorical_accuracy: 0.3943 - val_loss: 557.9153 - val_sparse_categorical_accuracy: 0.1817\n",
      "Epoch 2/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - loss: 1.0299 - sparse_categorical_accuracy: 0.6279 - val_loss: 18.5185 - val_sparse_categorical_accuracy: 0.2370\n",
      "Epoch 3/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - loss: 0.9177 - sparse_categorical_accuracy: 0.6563 - val_loss: 6.2340 - val_sparse_categorical_accuracy: 0.3317\n",
      "Epoch 4/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 57ms/step - loss: 0.8748 - sparse_categorical_accuracy: 0.6678 - val_loss: 5.6468 - val_sparse_categorical_accuracy: 0.3010\n",
      "Epoch 5/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - loss: 0.8640 - sparse_categorical_accuracy: 0.6694 - val_loss: 2.2744 - val_sparse_categorical_accuracy: 0.4997\n",
      "Epoch 6/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - loss: 0.8504 - sparse_categorical_accuracy: 0.6758 - val_loss: 1.7127 - val_sparse_categorical_accuracy: 0.5243\n",
      "Epoch 7/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 59ms/step - loss: 0.8236 - sparse_categorical_accuracy: 0.6874 - val_loss: 1.1562 - val_sparse_categorical_accuracy: 0.6224\n",
      "Epoch 8/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - loss: 0.8165 - sparse_categorical_accuracy: 0.6914 - val_loss: 1.2693 - val_sparse_categorical_accuracy: 0.5999\n",
      "Epoch 9/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - loss: 0.8059 - sparse_categorical_accuracy: 0.6981 - val_loss: 1.3452 - val_sparse_categorical_accuracy: 0.5857\n",
      "Epoch 10/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - loss: 0.7879 - sparse_categorical_accuracy: 0.7051 - val_loss: 1.1172 - val_sparse_categorical_accuracy: 0.6212\n",
      "\n",
      "Эксперимент с filters=32, kernel_size=(7, 7), activation='selu'\n",
      "Epoch 1/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 125ms/step - loss: 1.4576 - sparse_categorical_accuracy: 0.4408 - val_loss: 768.5447 - val_sparse_categorical_accuracy: 0.2969\n",
      "Epoch 2/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - loss: 0.9568 - sparse_categorical_accuracy: 0.6637 - val_loss: 2.5969 - val_sparse_categorical_accuracy: 0.4860\n",
      "Epoch 3/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - loss: 0.8597 - sparse_categorical_accuracy: 0.6894 - val_loss: 1.8410 - val_sparse_categorical_accuracy: 0.6267\n",
      "Epoch 4/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - loss: 0.7924 - sparse_categorical_accuracy: 0.7104 - val_loss: 2.6594 - val_sparse_categorical_accuracy: 0.2310\n",
      "Epoch 5/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 59ms/step - loss: 0.7745 - sparse_categorical_accuracy: 0.7134 - val_loss: 1.0143 - val_sparse_categorical_accuracy: 0.6361\n",
      "Epoch 6/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - loss: 0.7217 - sparse_categorical_accuracy: 0.7372 - val_loss: 2.3090 - val_sparse_categorical_accuracy: 0.1886\n",
      "Epoch 7/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - loss: 0.7180 - sparse_categorical_accuracy: 0.7355 - val_loss: 1.0685 - val_sparse_categorical_accuracy: 0.5974\n",
      "Epoch 8/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - loss: 0.6848 - sparse_categorical_accuracy: 0.7500 - val_loss: 1.2187 - val_sparse_categorical_accuracy: 0.6112\n",
      "Epoch 9/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - loss: 0.6701 - sparse_categorical_accuracy: 0.7532 - val_loss: 1.1692 - val_sparse_categorical_accuracy: 0.5605\n",
      "Epoch 10/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - loss: 0.6334 - sparse_categorical_accuracy: 0.7672 - val_loss: 0.9905 - val_sparse_categorical_accuracy: 0.6077\n",
      "\n",
      "Эксперимент с filters=64, kernel_size=(3, 3), activation='relu'\n",
      "Epoch 1/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 236ms/step - loss: 1.3895 - sparse_categorical_accuracy: 0.4939 - val_loss: 2479.0693 - val_sparse_categorical_accuracy: 0.1874\n",
      "Epoch 2/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 43ms/step - loss: 0.8867 - sparse_categorical_accuracy: 0.6684 - val_loss: 4.5078 - val_sparse_categorical_accuracy: 0.5451\n",
      "Epoch 3/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 43ms/step - loss: 0.7864 - sparse_categorical_accuracy: 0.7086 - val_loss: 1.4031 - val_sparse_categorical_accuracy: 0.6194\n",
      "Epoch 4/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 43ms/step - loss: 0.7123 - sparse_categorical_accuracy: 0.7371 - val_loss: 1.5347 - val_sparse_categorical_accuracy: 0.5771\n",
      "Epoch 5/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 43ms/step - loss: 0.6855 - sparse_categorical_accuracy: 0.7480 - val_loss: 1.0221 - val_sparse_categorical_accuracy: 0.6153\n",
      "Epoch 6/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 43ms/step - loss: 0.6657 - sparse_categorical_accuracy: 0.7580 - val_loss: 1.2185 - val_sparse_categorical_accuracy: 0.5768\n",
      "Epoch 7/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 43ms/step - loss: 0.6331 - sparse_categorical_accuracy: 0.7688 - val_loss: 0.9737 - val_sparse_categorical_accuracy: 0.6486\n",
      "Epoch 8/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 43ms/step - loss: 0.6040 - sparse_categorical_accuracy: 0.7785 - val_loss: 1.1667 - val_sparse_categorical_accuracy: 0.6179\n",
      "Epoch 9/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 43ms/step - loss: 0.5657 - sparse_categorical_accuracy: 0.7946 - val_loss: 1.2548 - val_sparse_categorical_accuracy: 0.5801\n",
      "Epoch 10/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 43ms/step - loss: 0.5454 - sparse_categorical_accuracy: 0.8000 - val_loss: 1.6765 - val_sparse_categorical_accuracy: 0.4193\n",
      "\n",
      "Эксперимент с filters=64, kernel_size=(3, 3), activation='linear'\n",
      "Epoch 1/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 112ms/step - loss: 1.5152 - sparse_categorical_accuracy: 0.4079 - val_loss: 168.5209 - val_sparse_categorical_accuracy: 0.2678\n",
      "Epoch 2/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 43ms/step - loss: 0.9890 - sparse_categorical_accuracy: 0.6343 - val_loss: 11.5184 - val_sparse_categorical_accuracy: 0.3312\n",
      "Epoch 3/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 43ms/step - loss: 0.8623 - sparse_categorical_accuracy: 0.6766 - val_loss: 2.4705 - val_sparse_categorical_accuracy: 0.5360\n",
      "Epoch 4/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 43ms/step - loss: 0.8469 - sparse_categorical_accuracy: 0.6832 - val_loss: 3.0729 - val_sparse_categorical_accuracy: 0.5103\n",
      "Epoch 5/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 44ms/step - loss: 0.8335 - sparse_categorical_accuracy: 0.6853 - val_loss: 2.3461 - val_sparse_categorical_accuracy: 0.4692\n",
      "Epoch 6/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 43ms/step - loss: 0.7982 - sparse_categorical_accuracy: 0.6976 - val_loss: 2.3218 - val_sparse_categorical_accuracy: 0.4833\n",
      "Epoch 7/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 43ms/step - loss: 0.8004 - sparse_categorical_accuracy: 0.6999 - val_loss: 1.7473 - val_sparse_categorical_accuracy: 0.5423\n",
      "Epoch 8/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 43ms/step - loss: 0.7819 - sparse_categorical_accuracy: 0.7081 - val_loss: 1.2472 - val_sparse_categorical_accuracy: 0.6104\n",
      "Epoch 9/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 43ms/step - loss: 0.7787 - sparse_categorical_accuracy: 0.7070 - val_loss: 1.3878 - val_sparse_categorical_accuracy: 0.5639\n",
      "Epoch 10/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 43ms/step - loss: 0.7680 - sparse_categorical_accuracy: 0.7112 - val_loss: 1.1070 - val_sparse_categorical_accuracy: 0.6105\n",
      "\n",
      "Эксперимент с filters=64, kernel_size=(3, 3), activation='selu'\n",
      "Epoch 1/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 114ms/step - loss: 1.5181 - sparse_categorical_accuracy: 0.4057 - val_loss: 522.5378 - val_sparse_categorical_accuracy: 0.0708\n",
      "Epoch 2/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 44ms/step - loss: 0.9055 - sparse_categorical_accuracy: 0.6710 - val_loss: 3.3562 - val_sparse_categorical_accuracy: 0.5568\n",
      "Epoch 3/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 44ms/step - loss: 0.7889 - sparse_categorical_accuracy: 0.7101 - val_loss: 1.6603 - val_sparse_categorical_accuracy: 0.4469\n",
      "Epoch 4/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 44ms/step - loss: 0.7451 - sparse_categorical_accuracy: 0.7277 - val_loss: 1.4637 - val_sparse_categorical_accuracy: 0.5643\n",
      "Epoch 5/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 44ms/step - loss: 0.6930 - sparse_categorical_accuracy: 0.7462 - val_loss: 0.9691 - val_sparse_categorical_accuracy: 0.6351\n",
      "Epoch 6/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 44ms/step - loss: 0.6521 - sparse_categorical_accuracy: 0.7615 - val_loss: 1.2945 - val_sparse_categorical_accuracy: 0.5395\n",
      "Epoch 7/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 44ms/step - loss: 0.6272 - sparse_categorical_accuracy: 0.7693 - val_loss: 2.0412 - val_sparse_categorical_accuracy: 0.3062\n",
      "Epoch 8/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 45ms/step - loss: 0.6302 - sparse_categorical_accuracy: 0.7699 - val_loss: 0.9576 - val_sparse_categorical_accuracy: 0.6757\n",
      "Epoch 9/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 44ms/step - loss: 0.6060 - sparse_categorical_accuracy: 0.7792 - val_loss: 0.9941 - val_sparse_categorical_accuracy: 0.6300\n",
      "Epoch 10/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 44ms/step - loss: 0.5619 - sparse_categorical_accuracy: 0.7939 - val_loss: 0.9272 - val_sparse_categorical_accuracy: 0.6817\n",
      "\n",
      "Эксперимент с filters=64, kernel_size=(7, 7), activation='relu'\n",
      "Epoch 1/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 545ms/step - loss: 1.6046 - sparse_categorical_accuracy: 0.4137 - val_loss: 449.7552 - val_sparse_categorical_accuracy: 0.1688\n",
      "Epoch 2/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 143ms/step - loss: 0.9668 - sparse_categorical_accuracy: 0.6457 - val_loss: 7.1364 - val_sparse_categorical_accuracy: 0.1405\n",
      "Epoch 3/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 143ms/step - loss: 0.8546 - sparse_categorical_accuracy: 0.6835 - val_loss: 4.0421 - val_sparse_categorical_accuracy: 0.1578\n",
      "Epoch 4/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 143ms/step - loss: 0.7966 - sparse_categorical_accuracy: 0.7100 - val_loss: 9.8285 - val_sparse_categorical_accuracy: 0.1795\n",
      "Epoch 5/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 143ms/step - loss: 0.7681 - sparse_categorical_accuracy: 0.7183 - val_loss: 1.1465 - val_sparse_categorical_accuracy: 0.5882\n",
      "Epoch 6/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 142ms/step - loss: 0.7309 - sparse_categorical_accuracy: 0.7304 - val_loss: 2.7811 - val_sparse_categorical_accuracy: 0.2878\n",
      "Epoch 7/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 143ms/step - loss: 0.6914 - sparse_categorical_accuracy: 0.7459 - val_loss: 1.3602 - val_sparse_categorical_accuracy: 0.5090\n",
      "Epoch 8/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 143ms/step - loss: 0.6822 - sparse_categorical_accuracy: 0.7492 - val_loss: 1.2302 - val_sparse_categorical_accuracy: 0.5338\n",
      "Epoch 9/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 143ms/step - loss: 0.6644 - sparse_categorical_accuracy: 0.7541 - val_loss: 1.7129 - val_sparse_categorical_accuracy: 0.4445\n",
      "Epoch 10/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 143ms/step - loss: 0.6162 - sparse_categorical_accuracy: 0.7734 - val_loss: 1.2595 - val_sparse_categorical_accuracy: 0.5659\n",
      "\n",
      "Эксперимент с filters=64, kernel_size=(7, 7), activation='linear'\n",
      "Epoch 1/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 209ms/step - loss: 1.6953 - sparse_categorical_accuracy: 0.3930 - val_loss: 47.7178 - val_sparse_categorical_accuracy: 0.4380\n",
      "Epoch 2/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 145ms/step - loss: 1.0733 - sparse_categorical_accuracy: 0.5993 - val_loss: 3.6238 - val_sparse_categorical_accuracy: 0.4755\n",
      "Epoch 3/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 145ms/step - loss: 0.9451 - sparse_categorical_accuracy: 0.6415 - val_loss: 1.7809 - val_sparse_categorical_accuracy: 0.5116\n",
      "Epoch 4/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 145ms/step - loss: 0.8900 - sparse_categorical_accuracy: 0.6616 - val_loss: 1.7697 - val_sparse_categorical_accuracy: 0.5423\n",
      "Epoch 5/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 145ms/step - loss: 0.8860 - sparse_categorical_accuracy: 0.6626 - val_loss: 2.0434 - val_sparse_categorical_accuracy: 0.5667\n",
      "Epoch 6/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 145ms/step - loss: 0.8580 - sparse_categorical_accuracy: 0.6755 - val_loss: 1.5398 - val_sparse_categorical_accuracy: 0.4968\n",
      "Epoch 7/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 145ms/step - loss: 0.8484 - sparse_categorical_accuracy: 0.6805 - val_loss: 1.5691 - val_sparse_categorical_accuracy: 0.5474\n",
      "Epoch 8/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 145ms/step - loss: 0.8224 - sparse_categorical_accuracy: 0.6891 - val_loss: 1.4095 - val_sparse_categorical_accuracy: 0.5261\n",
      "Epoch 9/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 145ms/step - loss: 0.8244 - sparse_categorical_accuracy: 0.6894 - val_loss: 1.1795 - val_sparse_categorical_accuracy: 0.5849\n",
      "Epoch 10/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 145ms/step - loss: 0.8090 - sparse_categorical_accuracy: 0.6933 - val_loss: 1.1868 - val_sparse_categorical_accuracy: 0.5673\n",
      "\n",
      "Эксперимент с filters=64, kernel_size=(7, 7), activation='selu'\n",
      "Epoch 1/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 211ms/step - loss: 1.8264 - sparse_categorical_accuracy: 0.4164 - val_loss: 58.8885 - val_sparse_categorical_accuracy: 0.3204\n",
      "Epoch 2/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 145ms/step - loss: 0.9820 - sparse_categorical_accuracy: 0.6366 - val_loss: 5.6448 - val_sparse_categorical_accuracy: 0.3133\n",
      "Epoch 3/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 145ms/step - loss: 0.8699 - sparse_categorical_accuracy: 0.6771 - val_loss: 2.6633 - val_sparse_categorical_accuracy: 0.4444\n",
      "Epoch 4/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 145ms/step - loss: 0.8416 - sparse_categorical_accuracy: 0.6890 - val_loss: 1.1708 - val_sparse_categorical_accuracy: 0.6123\n",
      "Epoch 5/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 145ms/step - loss: 0.8052 - sparse_categorical_accuracy: 0.7017 - val_loss: 1.0623 - val_sparse_categorical_accuracy: 0.6128\n",
      "Epoch 6/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 146ms/step - loss: 0.7577 - sparse_categorical_accuracy: 0.7223 - val_loss: 1.3510 - val_sparse_categorical_accuracy: 0.4986\n",
      "Epoch 7/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 146ms/step - loss: 0.7189 - sparse_categorical_accuracy: 0.7368 - val_loss: 1.7851 - val_sparse_categorical_accuracy: 0.4016\n",
      "Epoch 8/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 146ms/step - loss: 0.6883 - sparse_categorical_accuracy: 0.7476 - val_loss: 1.7951 - val_sparse_categorical_accuracy: 0.5389\n",
      "Epoch 9/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 145ms/step - loss: 0.7015 - sparse_categorical_accuracy: 0.7428 - val_loss: 1.1003 - val_sparse_categorical_accuracy: 0.5749\n",
      "Epoch 10/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 145ms/step - loss: 0.6534 - sparse_categorical_accuracy: 0.7593 - val_loss: 0.9164 - val_sparse_categorical_accuracy: 0.6664\n"
     ]
    }
   ],
   "source": [
    "for filters, kernel_size, activation in itertools.product(\n",
    "    filters_list, kernel_size_list, activation_list\n",
    "):\n",
    "    print(\n",
    "        f\"\\nЭксперимент с filters={filters}, kernel_size={kernel_size}, activation='{activation}'\"\n",
    "    )\n",
    "\n",
    "    model_masked_unet = masked_unet(\n",
    "        NUM_CLASSES,\n",
    "        (IMG_HEIGHT, IMG_WIDTH, 3),\n",
    "        filters=filters,\n",
    "        kernel_size=kernel_size,\n",
    "        activation=activation,\n",
    "    )  # Создаем модель\n",
    "\n",
    "    history = model_masked_unet.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=16,\n",
    "        validation_data=(x_val, y_val),\n",
    "        verbose=1,\n",
    "    )  # Обучаем модель\n",
    "\n",
    "    train_accuracy = history.history[\"sparse_categorical_accuracy\"][-1]\n",
    "    val_accuracy = history.history[\"val_sparse_categorical_accuracy\"][-1]\n",
    "    del model_masked_unet\n",
    "    del history\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    experiment_results.append(\n",
    "        {\n",
    "            \"filters\": filters,\n",
    "            \"kernel_size\": kernel_size,\n",
    "            \"activation\": activation,\n",
    "            \"train_accuracy\": train_accuracy,\n",
    "            \"val_accuracy\": val_accuracy,\n",
    "        }\n",
    "    )  # Добавляем результаты экспериментов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вывод сравнительной таблицы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filters</th>\n",
       "      <th>kernel_size</th>\n",
       "      <th>activation</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>val_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32</td>\n",
       "      <td>(3, 3)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.806657</td>\n",
       "      <td>0.689092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32</td>\n",
       "      <td>(3, 3)</td>\n",
       "      <td>linear</td>\n",
       "      <td>0.728444</td>\n",
       "      <td>0.635474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32</td>\n",
       "      <td>(3, 3)</td>\n",
       "      <td>selu</td>\n",
       "      <td>0.800199</td>\n",
       "      <td>0.571713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32</td>\n",
       "      <td>(7, 7)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.771031</td>\n",
       "      <td>0.544441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>(7, 7)</td>\n",
       "      <td>linear</td>\n",
       "      <td>0.702281</td>\n",
       "      <td>0.621223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>32</td>\n",
       "      <td>(7, 7)</td>\n",
       "      <td>selu</td>\n",
       "      <td>0.761336</td>\n",
       "      <td>0.607680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>64</td>\n",
       "      <td>(3, 3)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.799453</td>\n",
       "      <td>0.419250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>64</td>\n",
       "      <td>(3, 3)</td>\n",
       "      <td>linear</td>\n",
       "      <td>0.714001</td>\n",
       "      <td>0.610479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>64</td>\n",
       "      <td>(3, 3)</td>\n",
       "      <td>selu</td>\n",
       "      <td>0.790500</td>\n",
       "      <td>0.681658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>64</td>\n",
       "      <td>(7, 7)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.765020</td>\n",
       "      <td>0.565912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>64</td>\n",
       "      <td>(7, 7)</td>\n",
       "      <td>linear</td>\n",
       "      <td>0.695846</td>\n",
       "      <td>0.567316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>64</td>\n",
       "      <td>(7, 7)</td>\n",
       "      <td>selu</td>\n",
       "      <td>0.758767</td>\n",
       "      <td>0.666408</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    filters kernel_size activation  train_accuracy  val_accuracy\n",
       "0        32      (3, 3)       relu        0.806657      0.689092\n",
       "1        32      (3, 3)     linear        0.728444      0.635474\n",
       "2        32      (3, 3)       selu        0.800199      0.571713\n",
       "3        32      (7, 7)       relu        0.771031      0.544441\n",
       "4        32      (7, 7)     linear        0.702281      0.621223\n",
       "5        32      (7, 7)       selu        0.761336      0.607680\n",
       "6        64      (3, 3)       relu        0.799453      0.419250\n",
       "7        64      (3, 3)     linear        0.714001      0.610479\n",
       "8        64      (3, 3)       selu        0.790500      0.681658\n",
       "9        64      (7, 7)       relu        0.765020      0.565912\n",
       "10       64      (7, 7)     linear        0.695846      0.567316\n",
       "11       64      (7, 7)       selu        0.758767      0.666408"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.DataFrame(experiment_results)\n",
    "display(results_df)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "global-cKEoLTwX-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
